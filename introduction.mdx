---
title: Introduction
---

## What is LLM-Q?

Project LLM-Q is an upcoming set of services for managing and controlling your LLM features born out of our experiences building LLM applications.

The fundamentals of building LLM applications are different to those of the past, and new tools are needed to:

- **Monitor** - Monitor and alert on the cost, latency, and quality of your application features in the high-variance world of LLM features.
- **Continuous deployment and evaluation** - Safely rollout new model / prompt versions and evaluate their performance from production usage and user feedback.
- **Train** - Collect and refine relevant prediction data from production for fine-tuning and training.
- **Execute** - Run async jobs such as model backtesting, batch processing of user data, and more with a token-aware scheduling system.

## Why choose LLM-Q?

- **Built for production** - Our highest priority is to ensure we provide you the tools to safely monitor and deploy LLM features, while minimizing any potential impact we could have on your service. By default our monitoring tools do not sit between you and your LLM API. We also design our services to have safe defaults and fallbacks, so that even in the case of an outsage on our side, your system would continue to work while we fix it.

- **Designed for privacy** - You have full control over what data is collected and stored by our services. We also provide straightforward mechanisms out of the box to comply with GDPR and other privacy regulations.

- **LLM-first design** - Our design considerations are created with LLM applications in mind, and we account for the non-determinism and high variance in job sizes and latency that you experience when working with LLMs.

- **Easy integration** - We provide flexible integration options depending on the level of detail you require. Start with a simple drop in replacement for your LLM SDK, and have the option to use our prompt management system, or add custom tracing for more complex parts of your application.

### Services

We'll be working on two services, starting out with Monitor and following up with Run.

<CardGroup cols={2}>
  <Card title="Monitor" icon="pen-to-square">
    See, manage, control the costs and deployment of your LLM apps features.
  </Card>
  <Card title="Run" icon="image">
    Execute your batch jobs with our LLM-first scheduler, designed for optimal
    throughput and cost savings.
  </Card>
</CardGroup>

### Why LLM-Q Monitor?

With Monitor, we provide everything you need to safely operate your LLM application in production.

- **Logging** - Collect all the data you need about your production inference calls, which is stored to explore, filter, and export.
- **Analytics** - Understand the magnitude and distribution of your applications latency, cost, and quality with our dashboard.
- **Alerting** - Use our integrations to give scheduled reports or set up alerts to maintain your SLOs.
- **Prompt Management** - Version control and share your model / prompt configurations with your team.
- **A/B testing and percentage rollouts** - Safely rollout and evaluate new model / prompt configurations.
- **Collect user feedback** - An API to collect user feedback for predictions and traces to help you with evaluation.
