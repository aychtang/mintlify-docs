---
title: "JS/TS Quickstart - Monitor"
description: "Start safely deploying and monitoring your LLM app features with procedural rollouts"
---

## Get started in 1 minute

<Steps titleSize="h2">
  <Step title="Drop in OpenAI SDK installation">
    Get started in 1 minute with our drop-in SDK installation for your app.

    ```ts
    import OpenAI from "codethread/OpenAI";

    const openAi = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      codeThreadApiKey: process.env.CODETHREAD_API_KEY,
    });

    const generation = await openAi.chat.completions.create({
      config,
      messages,
    });
    ```

  </Step>
  <Step title="Add additional metadata">
    Add the information you need to your LLM API calls that to help understand your users and how they are using your app.

        ```ts
        import OpenAI from "codethread/OpenAI";
        import User from "@/domain/users";

        const openAi = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
          codeThreadApiKey: process.env.CODETHREAD_API_KEY,
        });

        const user = await new User(id);

        const generation = await openAi.chat.completions.create({
          config,
          messages,
          properties: {
            _userId: user.id,
            _parentId: user.teamId,
            location: 'app/routes/location'
          }
        });
        ```
        <Tip>You can add whatever custom fields you need, but there are a set of indexed properties which are prefixed by an underscore.</Tip>

  </Step>
  <Step title="Get visibility of your data in the dashboard">
    See your data in the LLM dashboard and get insights into how your users are interacting with your app. We provide dashboards, graphs, monitors, and alerts on the following:
    - **User activity** - understand usage and user satisfaction through prediction logging, user feedback collection, and automated eval tools.
    - **Costs** - visualize and monitor your LLM costs, we provide graphs which help you understand the distribution of token usage and costs for each of your prompts.
    - **Performance** - see the latency and throughput of your LLM calls.
    - **Quality** - monitor and visualize the quality of your model, we provide graphs which show latency and throughput for each prompt that is running within your application.

  </Step>
</Steps>

## Start using managed prompts

Use managed prompts to get full version control and history of your prompts, making it easier to configure prompt and model settings. Hereâ€™s how you can start integrating managed prompts into your application:

```ts
import OpenAI from "codethread/OpenAI";
import User from "@/domain/users";

const openAi = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  codeThreadApiKey: process.env.CODETHREAD_API_KEY,
});

const user = await new User(id);

const generation = await openAi.chat.completions.create({
  // Config and prompt defined in your code can be used as a
  // fallback during service unavailability:
  config,
  messages,
  properties: {
    _userId: user.id,
    _parentId: user.teamId,
    location: 'app/routes/location'
  }
  codeThreadMeta: {
    promptName: "My Prompt Name in CodeThread Cloud",
    promptInputs: {
      user_id: user.id,
    },
  },
});
```

## Set up procedural rollouts of new model and prompt configurations

Once you are using managed prompts, you can set up a rollout of new model and prompt configurations.

<Tip>
  Learn more about how our percentage rollouts work{" "}
  <a href="/how/percentage-rollouts">here</a>.
</Tip>

## Collect user feedback for your app's generations

Collect user feedback for your app's generations with our feedback collection API.

```ts
import CodeThread from "codethread";
import OpenAI from "codethread/OpenAI";

const openAi = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  codeThreadApiKey: process.env.CODETHREAD_API_KEY,
});

const generation = await openAi.chat.completions.create({
  config,
  messages,
});

// Send user feedback for a generation:
await CodeThread.feedback.post({
  id: generation.codeThreadId,
  feedback: {
    value: 0,
    message: "Did not understand the example..",
  },
});
```

## Add custom tracing to your app

Set up tracing to help you understand more complex interactions within your app like RAG enabled features, prompt chains, and more.

```ts
import CodeThread from "codethread";
import OpenAI from "codethread/OpenAI";
import User from "@/domain/users";

const openAi = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  codeThreadApiKey: process.env.CODETHREAD_API_KEY,
});

const trace = CodeThread.trace({ name: "RAG Procedure" });

const span = trace.span({
  name: "Get top docs for user",
});

const user = await User(id);
span.event({
  name: "User fetched",
  properties: { user: user.toJSON() },
});

const docs = await User.getTopDocs(user.id);
span.event({
  name: "Documents fetched",
  properties: { docs },
});
span.close();

const generation = await openAi.chat.completions.create({
  config,
  messages: prompt(user, docs),
  codeThreadMeta: {
    traceId: trace.id,
  },
});

trace.close();
```
